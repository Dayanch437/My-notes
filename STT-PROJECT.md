~~~python
pip install git+https://github.com/openai/whisper.git
pip install torchaudio
~~~

~~~python 
/stt-project/
â”œâ”€â”€ samples/
â”‚   â”œâ”€â”€ turkmen1.wav
â”‚   â””â”€â”€ turkmen2.wav
â”œâ”€â”€ transcribe.py
~~~

~~~python 
ffmpeg -i input.mp3 -ar 16000 -ac 1 samples/turkmen1.wav
~~~

transcribe.py
~~~python 
import whisper
import os

model = whisper.load_model("base")  # or "small", "medium", "large"

audio_dir = "samples"

for filename in os.listdir(audio_dir):
    if filename.endswith(".wav"):
        path = os.path.join(audio_dir, filename)
        print(f"\nğŸ”Š Transcribing: {filename}")
        result = model.transcribe(path, language="tk")  # "tk" = Turkmen
        print("ğŸ“ Transcript:", result["text"])

~~~

for my own 
### 1. ğŸ“ **Prepare Dataset**

You need a large dataset with:

- `.wav` files (16kHz, mono recommended)
    
- Matching transcripts (in `.txt`, `.csv`, or `.json`)

~~~arduino
/data/
  â”œâ”€â”€ audio/
  â”‚     â”œâ”€â”€ 001.wav
  â”‚     â”œâ”€â”€ 002.wav
  â””â”€â”€ transcriptions.csv
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ file     â”‚ text               â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ 001.wav  â”‚ salam dostlarym    â”‚
        â”‚ 002.wav  â”‚ men bararyn        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
~~~

### 2. ğŸ§  Choose a Model Architecture

Start with one of these:

- `wav2vec 2.0` (by Meta)
    
- `QuartzNet`, `Conformer`, or `DeepSpeech` (by Mozilla)
    

ğŸŸ¢ I recommend **wav2vec 2.0** if you want SOTA accuracy.

	1. ğŸ§ª Preprocess Audio

~~~bash
ffmpeg -i input.amr -ar 16000 -ac 1 output.wav
~~~

~~~bash
for f in *.amr; do
  ffmpeg -i "$f" -ar 16000 -ac 1 "${f%.amr}.wav"
done
~~~

4. ğŸ› ï¸ Install Tools
~~~bash
pip install torchaudio transformers datasets jiwer
~~~


